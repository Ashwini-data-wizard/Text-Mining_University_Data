# Text-Mining_University_Data
University Guidelines on Integrating Generative AI
## Scope
The purpose of this project was to investigate a specific research question proposed by Dr. Bei Yu: using text mining techniques, what can we determine about the state of generative AI in American universities from guidelines released on the websites of the top 100 American universities?

## Our hypotheses were:
1. h1: Most large universities will have guidelines about the use of generative AI for faculty/staff and students publicly available on their websites.
2. h2: We are likely to see themes of data privacy, academic integrity, and AI reliability across the guidelines.
3. h3: We are likely to see a focus on information literacy in the guidelines.
4. h4: Observations of existing guidelines can be used to help other higher education institutions formulate their own guidelines in the future.

## Data Collection
We retrieved our list of universities from US News & World Report’s 2024 “National Universities” list (US News & World Report, 2023), which gave us a total of 104 universities due to the ranking system they use. We then reconciled the universities’ names to Wikidata entities with OpenRefine’s reconciliation service and pulled relevant descriptive information through OpenRefine’s Wikidata API: P131 “located in the administrative territorial entity” to yield a town or city with a further P131 queried from the result to yield the state, P625 “coordinate location,” P2643 “Carnegie Classification of Institutions of Higher Education,” and P856 “official website.” Initial data from P2196 “students count” proved to be less up-to-date than Wikipedia, so Wikipedia values were incorporated into the dataset instead. 

The P856 “official website” data was used to formulate our Google searches. We used the format, “site: “syracuse.edu” generative ai guidelines” as our basic search; this yielded results with high effectiveness. 19 universities did not have guidelines that we could locate, but 85 did.

We then manually copied the text of relevant web pages and pasted it into a spreadsheet for later import into our Colab Notebook. This approach alleviated the issue of having different website formats such as multiple tabs on the same topic or drop-down sections of text that made automated scraping approaches like Beautiful Soup or Selenium too time-consuming for us to code for this endeavor. 

Other challenges to our data collection included identifying what might be considered “guidelines” versus what might be considered a “statement” or a “policy.” We found that most information we located was clearly guidelines and not formal policy. The differentiation between different colleges within universities and different campuses within universities also posed challenges, as not all institutions are organized the same way nor share information the same way. We also noted that there were sometimes multiple pages for the same topic, divided either by subtopic or by intended audience. Access restrictions are likely; some guidelines we did not locate may have been shared on a university intranet but not publicly.

### Universities
The universities represented among the top 100 ranks were not widely distributed based on criteria of Carnegie rankings or size classification. There were 84 R1 universities, representing 81% of our data; 13 R2 universities, representing 13% of our data; 3 R3 institutions representing 3% of our data; and 4 M1 universities, representing 4% of our data. See Visualization 1.

In considering size, we had to divide the Carnegie size category of “large” into “large” and “very large” categories. Based on the modified divisions, we had 1 small university (1,000-2,999 students) representing 1% of our data, 17 medium universities (3,000-9,999 students) representing 16% of our data, 47 large universities (10,000-29,999 students) representing 45% of our data, and 39 very large universities (30,000+ students) representing 38% of our data. See Visualization 2.

We also analyzed where the universities were located in the United States and found that they represented the overall geographic distribution of American universities fairly well. See Visualization 3.

## Text Mining
We chose to analyze the guidelines through 3 separate approaches: sentiment analysis, common topics, and K-means clustering. Our goal was to experiment with what would yield the richest results, as well as to analyze what we could learn from each of the results.

### Corpus Linguistics
Preliminary investigation with corpus linguistics techniques did not yield particularly illuminating insights, but some interesting observations were made. When examining a wordcloud by unigram frequency, “ai” and “use” are unsurprisingly the foci. “Students” is far more prominent than “instructor,” which illustrates a focus in the guidelines on students rather than faculty. Quite interestingly, “tools” appears more frequently than expected, suggesting that universities are framing generative AI as tools for learning. See Visualization 4.

A wordcloud of bigram frequency shows us “ai tools,” confirming the earlier unigram analysis that generative AI is being framed as a tool. We also see “academic integrity” and “academic misconduct,” illustrating the concerns of universities regarding how these tools might be used. See Visualization 5.

### Sentiment Analysis
We used a HuggingFace default model and distilbert-base-uncased-finetuned model to analyze the sentiments of the university guidelines, identifying which had overall positive language about the usage of generative AI and which had overall negative language. The challenge we experienced in this goal was that the guidelines from the universities were too large. We modified the code to divide each statement into chunks of 512 tokens each, the limit of the model. We then assigned each chunk a sentiment, and the university was analyzed as either positive or negative by the majority sentiments of those chunks.

51 of the universities expressed a negative sentiment, while 27 expressed a positive sentiment. See Visualization 6 and Output 1.

In comparison with HuggingFace, we also tested a VADER model for sentiment analysis. It was able to predict positive sentiments with respect to ethics, collaboration, privacy, and innovation. However, it was only able to predict four universities with a negative stance toward the use of generative AI. Only two universities were labeled in common with the results of the HuggingFace sentiment analysis. When comparing the results, our HuggingFace model produced a better division of sentiments indicated in the text, possibly due to differences in the fundamental functions of HuggingFace and VADER. VADER is lexicon and rule based, which enables it to provide a preliminary understanding of the text based on the polarity scores of the words. On the other hand, HuggingFace requires fine tuning of the pre-trained model on a dataset specific to your domain or task, which  allows the model to adapt to the nuances of your data and, in cases such as ours, produce more successful results for analysis. See Visualization 7.

### Common topics
We utilized LDA to find common topics that exist between each of the university's guidelines. We pre-processed by removing numbers, punctuation, stop words, and custom stop words like "ai.” From the preprocessed data, we generated the top 15 topics. This was then given as an input with a prompt to GPT to create relevant titles based on those topics; GPT was also prompted to assign the ACRL information literacy Frames (ACRL 2015) across the groupings in order to better understand how the topics reflected information literacy’s importance in the guidelines. See Output 2 for the topics with associated Frames and the universities from whose guidelines the topics were drawn.

We also tried BERTopic for this investigation, but faced issues that we couldn’t debug. Only three topics would generate, which was insufficiently large for our analysis. An increased parameter for a higher number of topics returned none. Because of the time constraints of this project being completed for a course, we opted to discontinue this approach and instead focus on the LDA analysis.

### K-means clustering 
When pursuing a k-means clustering approach, handling the lengthy texts of the guidelines for each university was difficult. The results gave us overly large clusters with many of them being irrelevant or incoherent. The strategy used to overcome this was to transform the extensive text into indexed paragraphs and perform clustering on those paragraph segments. These indexes were then used to trace clusters back to their respective universities.

The clustering efficiently categorized the universities based on the predominant themes in their guidelines and can provide insights for creating new policies tailored to align with any or each of the topics. We produced five distinct clusters which were analyzed by commonality. These clusters were “Ethics-Centric," "Research-Driven," "Comprehensive," "Privacy-Focused," and “Prohibitive and Restrictive.” See Visualization 8 for the scatterplot; note that the guidelines focused on restricted use are significantly separated from the rest, suggesting a marked difference between restrictive guidelines and guidelines focused on facilitating responsible use.

## AI & Information Literacy 
Interestingly, there appears to be no relationship between university Carnegie rankings, size classifications, geographic locations and their guidelines, including whether or not they had guidelines, if they were positive or negative in sentiment analysis, and what topics they expressed. Although, as aforementioned, our data was significantly skewed toward large and very large R1 universities, this preliminary work suggests that the desire and need for guidelines and their content is not relevant to only one type of university, but rather relevant to all of higher education. In this section, we will discuss our preliminary literature review on generative AI integration in higher education and how it relates to our findings.

To understand the intersection of AI and information literacy, we must first understand the intersection of AI and other domain literacies. Long and Magerko (2020) define AI literacy “as a set of competencies that enables individuals to critically evaluate AI technologies; communicate and collaborate effectively with AI; and use AI as a tool online, at home, and in the workplace” (2). The concept of “literacy” as core, foundational competencies is utilized here on the specific domain of AI knowledge. They argue that “Digital literacy is a prerequisite for AI literacy, as individuals need to understand how to use computers to make sense of AI” (2). They note that computational literacy and scientific literacy can help to inform aspects of AI literacy, but do not consider them to be foundational. They also note that data literacy is “closely related to the AI field of machine learning, and therefore certain data literacy competencies overlap with AI literacy competencies” (2). They do not, however, mention information literacy at any point.

In their broad review of scholarship on AI literacy, Ng et al. (2021) note that there are a variety of ways that scholars define the concept of AI literacy, which influences how educators are communicating its foundational concepts in pedagogical settings. They outline four common aspects of AI literacy across the scholarship that they reviewed: to “know and understand AI,” to “use and apply AI,” to “evaluate and create AI,” and to understand concepts of “AI ethics” (4). They observe that “A possible reason that existing AI literacy studies focused more on general skills and knowledge about AI is that AI literacy is a set of fundamental skills and abilities in helping [...people…] to acquire, construct and apply knowledge. They may not necessarily handle how to abstract and decompose AI problems, nor build AI applications; instead, they need to know the basic concepts and use AI ethically” (4). The only time they mention information literacy is when giving a brief list of other common literacies that form the ecosystem that AI literacy is a part of, many of which, they note, are used as contextual information in scholarship to anchor AI literacy concepts (9).

In her discussion of teaching with AI literacy, Pretorius (2023) argues that she “believe[s] what is missing from [...] academic integrity discussions regarding generative AI is the fact that assessment regimes focus predominantly on the product of learning but neglect the importance of the learning process” and that generative AI “should be considered as an aide with the intellectual work of the user residing in the choice of an appropriate prompt, assessment of the suitability of the output, and further modification of that prompt if the output does not seem suitable” (3). Her points about “the intellectual work of the user” reflect the basic concepts of AI literacy discussed throughout AI literacy scholarship, but she highlights the importance of generative AI’s relationship to the learning process, not just the learning product. ACRL (2015) defines information literacy as “the set of integrated abilities encompassing the reflective discovery of information, the understanding of how information is produced and valued, and the use of information in creating new knowledge and participating ethically in communities of learning” (26), which dovetails with Pretorius’ arguments about teaching with AI literacy.

Carol Kuhlthau’s influential Information Search Process (n.d.) in the domain of information literacy outlines a user’s process in six steps: “task initiation,” “selection,” “exploration,” “focus formulation,” “collection,” and “presentation”. In Pretorius’ words, this is all part of “the intellectual work of the user,” that learning process that a user goes through in order to produce an output, Kuhlthau’s “presentation.” Generative AI can assist users as a tool for any of these steps, from helping them choose a topic, to selecting search terms, to honing an idea, to creating a presentation. University guidelines that are intended to deter students from using generative AI fail to acknowledge the need for students to learn how to use AI tools and develop AI literacy, which we believe cannot be developed without information literacy, despite the fact that the majority of scholarship on AI literacy seems to ignore or gloss over the importance of information literacy’s relationship to AI. The “problem based” or “enquiry based approach to learning” summarized by Hepworth (2000) is based on the concept that “information literacy is taught and absorbed while the students solve problems associated with their discipline or profession” (28). These activities from an information literacy perspective are what helps students develop the basic competencies of AI literacy when applied to the domain of AI. Lo (2023) uses information literacy-based framework for prompt engineering that he calls “CLEAR” to teach students about information literacy and AI; the prompts are “concise, logical, explicit, adaptive, and reflective” (1).

Smith and Matteson (2018) outline the basic idea of literacy as that which “enables the use of recorded information, as do the tools and skills required for its creation, dissemination, and use” (73). They define “information tools” as “mechanisms that make information accessible” (73); the examples that they give do not include generative AI, but generative AI does indeed fit their definition of an information tool. They underscore the importance of context in the information seeking and use processes and argue that “the judgement processes of information literacy are weakened and their development compromised by the lack of easily accessible and useful context representations” (80). Generative AI, broadly, is notably lacking context for the information it imparts on its users, and typically only limited context can be gained by prompting it to include that information, another aspect in which information literacy is critical to AI literacy.

Our observations of the guidelines collected in our dataset suggests that most of the universities that were included have produced guidelines that understand AI literacy is an important aspect of student learning that must be included in the university's pedagogical responsibilities. Scott-Branch et al. (2023) point out that “faculty, students, and staff must comprehend the pros and cons of AI and other generative tools for ethical and effective usage and to fully harness their power” (1). This reflects the previous outlines of basic AI literacy concepts and supports our observation that even with the small number of restrictive policies in the k-means clusters, we still saw a high number of universities with overall negative sentiments about generative AI; we hypothesize that this is because at this time they are more wary of the cons than they are hopeful for the pros, yet they are aware enough of generative AI’s lasting impact that they are incorporating it anyway. Scott-Branch et al. also argue that libraries in particular “have the potential to make the most impact by championing the intersection of AI, digital, and information literacy. They can achieve this by developing educational resources and modules that teach individuals how to utilize emerging technologies with advanced levels of proficiency effectively” (2). 
Libraries,  of course, are not the only parts of higher education systems that can provide this necessary education, nor should they be the only place in which students are receiving it. As our LDA modeling and GPT labeling shows, many of the guidelines we analyzed already contain concepts related to the ACRL Information Literacy Frames: “Authority Is Constructed and Contextual,” “Information Creation as a Process,” “Information Has Value,” “Research as Inquiry,” “Scholarship as Conversation,” “Searching as Strategic Exploration” (ACRL 2015). Each of these frames are related to the diverse uses of generative AI in higher education and are reflected as such in our computational observations.

In relation to the intersection of AI literacy and information literacy sits topics of academic integrity. Cheating is a major concern that arose in each paper that we examined, as is the inability to evaluate a student’s work as original or AI, and even how to examine those boundaries of intellectual property. Eke (2023) notes that although there are ample opportunities for generative AI to violate academic integrity, “It is not sustainable to ban, reject or dismiss it. This is a technology that presents opportunities for teaching, research and innovation” (2). Plata et al. (2023) examined scholarship regarding generative AI and academic honor codes, identifying three themes across the literature: “Educating the Faculty and Students on the Impact of Generative AI on academic integrity,” “Enforcement at the University and Classroom Levels,” and “Encouraging the Exploration of the Capabilities of Generative AI.” Of particular interest to the rest of the literature we examined and the basis of this research project, they argue that students “should be familiar with the importance of academic integrity and ethical behavior, preventing academic dishonesty, and the consequences of academic misconduct. Students need clarification on what constitutes cheating, plagiarism, and academic misconduct, empowering them to report cases and make them aware of other ethical issues in the use of AI” (748). These concerns of student awareness are the entire reason that guidelines are being produced, as well as guidance for instructor awareness.

Literature surveys by McCabe et al. (2003) and Tatum (2022) on honor codes in higher education demonstrate that both faculty and students at institutions with honor codes have higher opinions of academic integrity as a principle and of academic integrity at their institutions than those at institutions without honor codes. This suggests that guidelines on the responsible use of generative AI and education about the topic with information and AI literacy, such as how institutions with honor codes educate incoming students about them, could potentially prove effective in creating an environment of responsible use where students maintain their own integrity and aid their peers in maintaining theirs.
 

## Conclusions and Further Research
In the course of this project, we have analyzed university guidelines on the use of generative AI and noted common patterns using text mining approaches.

There is much, much further research that can be done on this topic. As this was only a one-semester project, our observations need to be challenged by additional research, and a formal literature review on the topics of AI literacy, information literacy, and honor codes would be beneficial in creating a more comprehensive survey of existing scholarship.

As generative AI becomes more and more ‘a part of everyday life’ as we saw it described in paper after paper, we may see a shift from general guidelines to formal policies. Further examination would be needed to confirm it, but we developed a hypothesis that generative AI guidelines in syllabi preceded those at a university level, and that the guidelines at a university level are preceding formal policies about AI and academic integrity. This does not, however, according to our research, solve any problems; instead, universities must create cultures of responsibility, develop sustainable and integrated education on AI in their pedagogical practices for students, and support faculty in their use of and teaching with generative AI. Our observed patterns in topics and clusters can be used to help universities developing their own guidelines outline the basic points to build the rest of the guidelines around those foundational ideas that are common to many top-ranked universities.










Ethics statement
The authors of this report have no conflicts of interest with this content. Our data collection only involved the collection of publicly available data on university websites and publicly available data on Wikidata and Wikipedia. No sensitive information or PPI was collected at any point during this project. We do not foresee any ethical concerns with possible future uses of this project.
